{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MHkcIl4E-Fgf"
      },
      "source": [
        "# TF Ranking\n",
        "\n",
        "In this Notebook, we run through a simplified example to highlight some of the features of the TF Ranking library and demonstrate an end-to-end execution.\n",
        "\n",
        "The general recipe is a short list of four main steps:\n",
        "\n",
        "1.   Compose a function to **read** input data and prepare a Tensorflow Dataset;\n",
        "2.   Define a **scoring** function that, given a (set of) query-document feature vector(s), produces a score indicating the query's level of relevance to the document;\n",
        "3.   Create a **loss** function that measures how far off the produced scores from step (2) are from the ground truth; and,\n",
        "4.   Define evaluation **metrics**.\n",
        "\n",
        "A final step makes use of standard Tensorflow API to create, train, and evaluate a model.\n",
        "\n",
        "We have included in the TF Ranking library a default implementation of data readers (in the `tensorflow_ranking.data` module), loss functions (in `tensorflow_ranking.losses`), and popular evaluation metrics (in `tensorflow_ranking.metrics`) that may be further tailored to your needs as we shall show later in this Notebook.\n",
        "\n",
        "### Preparation\n",
        "\n",
        "In what follows, we will assume the existence of a dataset that is split into training and test sets and that are stored at `data/train.txt` and `data/test.txt` respectively. We further assume that the dataset is in the LibSVM format and lines in the training and test files are sorted by query ID -- an assumption that holds for many popular learning-to-rank benchmark datasets.\n",
        "\n",
        "We have included in our release a toy (randomly generated) dataset in the `data/` directory. However, to learn a more interesting model, you may copy your dataset of choice to the `data/` directory. Please ensure the format of your dataset conforms to the requirements above. Alternatively, you may edit this Notebook to plug in a customized input pipeline for a non-comformant dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1pn-thubzKJo"
      },
      "source": [
        "# Get Started with TF Ranking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZEvYpvoTzPyy"
      },
      "source": [
        "\u003ctable class=\"tfo-notebook-buttons\" align=\"left\"\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/ranking/blob/master/tensorflow_ranking/examples/tf_ranking_libsvm.ipynb\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" /\u003eRun in Google Colab\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca target=\"_blank\" href=\"https://github.com/tensorflow/ranking/blob/master/tensorflow_ranking/examples/tf_ranking_libsvm.ipynb\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" /\u003eView source on GitHub\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "\u003c/table\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "d_PrxvnLTO1i"
      },
      "source": [
        "### Dependencies and Global Variables\n",
        "\n",
        "Let us start by importing libraries that will be used throughout this Notebook. We also enable the \"eager execution\" mode for convenience and demonstration purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "height": 34
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 3135,
          "status": "ok",
          "timestamp": 1549671972484,
          "user": {
            "displayName": "",
            "photoUrl": "",
            "userId": ""
          },
          "user_tz": 480
        },
        "id": "8tleu4F7TtDu",
        "outputId": "df6cdb43-686f-42e1-fe29-9f2af06c8e4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/sh: pip: command not found\r\n"
          ]
        }
      ],
      "source": [
        "! pip install tensorflow_ranking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "kXPP5mzZswio"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_ranking as tfr\n",
        "\n",
        "tf.enable_eager_execution()\n",
        "tf.executing_eagerly()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0B09XkzZYKLV"
      },
      "source": [
        "Next, we will download a dummy dataset in LibSVM format.Note that you can replace these datasets with public or custom datasets. \n",
        "\n",
        "We also define some global parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "DFjEgfE0zxHT"
      },
      "outputs": [],
      "source": [
        "! wget -O \"/tmp/train.txt\" \"https://raw.githubusercontent.com/tensorflow/ranking/master/tensorflow_ranking/examples/data/train.txt\"\n",
        "! wget -O \"/tmp/test.txt\" \"https://raw.githubusercontent.com/tensorflow/ranking/master/tensorflow_ranking/examples/data/test.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "SDEhIqPSYJS1"
      },
      "outputs": [],
      "source": [
        "# Store the paths to files containing training and test instances.\n",
        "# As noted above, we will assume the data is in the LibSVM format\n",
        "# and that the content of each file is sorted by query ID.\n",
        "_TRAIN_DATA_PATH=\"/tmp/train.txt\"\n",
        "_TEST_DATA_PATH=\"/tmp/test.txt\"\n",
        "\n",
        "# Define a loss function. To find a complete list of available\n",
        "# loss functions or to learn how to add your own custom function\n",
        "# please refer to the tensorflow_ranking.losses module.\n",
        "_LOSS=\"pairwise_logistic_loss\"\n",
        "\n",
        "# In the TF-Ranking framework, a training instance is represented\n",
        "# by a Tensor that contains features from a list of documents\n",
        "# associated with a single query. For simplicity, we fix the shape\n",
        "# of these Tensors to a maximum list size and call it \"list_size,\"\n",
        "# the maximum number of documents per query in the dataset.\n",
        "# In this demo, we take the following approach:\n",
        "#   * If a query has fewer documents, its Tensor will be padded\n",
        "#     appropriately.\n",
        "#   * If a query has more documents, we shuffle its list of\n",
        "#     documents and trim the list down to the prescribed list_size.\n",
        "_LIST_SIZE=100\n",
        "\n",
        "# The total number of features per query-document pair.\n",
        "# We set this number to the number of features in the MSLR-Web30K\n",
        "# dataset.\n",
        "_NUM_FEATURES=136\n",
        "\n",
        "# Parameters to the scoring function.\n",
        "_BATCH_SIZE=32\n",
        "_HIDDEN_LAYER_DIMS=[\"20\", \"10\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZGJ6rRJyZmiB"
      },
      "source": [
        "### Input Pipeline\n",
        "\n",
        "The first step to construct an input pipeline that reads your dataset and produces a `tensorflow.data.Dataset` object. In this example, we will invoke a LibSVM parser that is included in the `tensorflow_ranking.data` module to generate a `Dataset` from a given file.\n",
        "\n",
        "We parameterize this function by a `path` argument so that the function can be used to read both training and test data files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "itrqULz5lubN"
      },
      "outputs": [],
      "source": [
        "def input_fn(path):\n",
        "  train_dataset = tf.data.Dataset.from_generator(\n",
        "      tfr.data.libsvm_generator(path, _NUM_FEATURES, _LIST_SIZE),\n",
        "      output_types=(\n",
        "          {str(k): tf.float32 for k in range(1,_NUM_FEATURES+1)},\n",
        "          tf.float32\n",
        "      ),\n",
        "      output_shapes=(\n",
        "          {str(k): tf.TensorShape([_LIST_SIZE, 1])\n",
        "            for k in range(1,_NUM_FEATURES+1)},\n",
        "          tf.TensorShape([_LIST_SIZE])\n",
        "      )\n",
        "  )\n",
        "\n",
        "  train_dataset = train_dataset.shuffle(1000).repeat().batch(_BATCH_SIZE)\n",
        "  return train_dataset.make_one_shot_iterator().get_next()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QdVFrZBIeWXL"
      },
      "source": [
        "### Scoring Function\n",
        "\n",
        "Next, we turn to the scoring function which is arguably at the heart of a TF Ranking model. The idea is to compute a relevance score for a (set of) query-document pair(s). The TF-Ranking model will use training data to learn this function.\n",
        "\n",
        "Here we formulate a scoring function using a feed forward network. The function takes the features of a single example (i.e., query-document pair) and produces a relevance score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "B7Ft1i2oieEY"
      },
      "outputs": [],
      "source": [
        "def example_feature_columns():\n",
        "  \"\"\"Returns the example feature columns.\"\"\"\n",
        "  feature_names = [\n",
        "      \"%d\" % (i + 1) for i in range(0, _NUM_FEATURES)\n",
        "  ]\n",
        "  return {\n",
        "      name: tf.feature_column.numeric_column(\n",
        "          name, shape=(1,), default_value=0.0) for name in feature_names\n",
        "  }\n",
        "\n",
        "def make_score_fn():\n",
        "  \"\"\"Returns a scoring function to build `EstimatorSpec`.\"\"\"\n",
        "\n",
        "  def _score_fn(context_features, group_features, mode, params, config):\n",
        "    \"\"\"Defines the network to score a documents.\"\"\"\n",
        "    del params\n",
        "    del config\n",
        "    # Define input layer.\n",
        "    example_input = [\n",
        "        tf.layers.flatten(group_features[name])\n",
        "        for name in sorted(example_feature_columns())\n",
        "    ]\n",
        "    input_layer = tf.concat(example_input, 1)\n",
        "\n",
        "    cur_layer = input_layer\n",
        "    for i, layer_width in enumerate(int(d) for d in _HIDDEN_LAYER_DIMS):\n",
        "      cur_layer = tf.layers.dense(\n",
        "          cur_layer,\n",
        "          units=layer_width,\n",
        "          activation=\"tanh\")\n",
        "\n",
        "    logits = tf.layers.dense(cur_layer, units=1)\n",
        "    return logits\n",
        "\n",
        "  return _score_fn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "e0gt2bu7kbtS"
      },
      "source": [
        "### Evaluation Metrics\n",
        "\n",
        "We have provided an implementation of popular Information Retrieval evalution metrics in the TF Ranking library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "YkU6o2QjkyXR"
      },
      "outputs": [],
      "source": [
        "def eval_metric_fns():\n",
        "  \"\"\"Returns a dict from name to metric functions.\n",
        "\n",
        "  This can be customized as follows. Care must be taken when handling padded\n",
        "  lists.\n",
        "\n",
        "  def _auc(labels, predictions, features):\n",
        "    is_label_valid = tf_reshape(tf.greater_equal(labels, 0.), [-1, 1])\n",
        "    clean_labels = tf.boolean_mask(tf.reshape(labels, [-1, 1], is_label_valid)\n",
        "    clean_pred = tf.boolean_maks(tf.reshape(predictions, [-1, 1], is_label_valid)\n",
        "    return tf.metrics.auc(clean_labels, tf.sigmoid(clean_pred), ...)\n",
        "  metric_fns[\"auc\"] = _auc\n",
        "\n",
        "  Returns:\n",
        "    A dict mapping from metric name to a metric function with above signature.\n",
        "  \"\"\"\n",
        "  metric_fns = {}\n",
        "  metric_fns.update({\n",
        "      \"metric/ndcg@%d\" % topn: tfr.metrics.make_ranking_metric_fn(\n",
        "          tfr.metrics.RankingMetricKey.NDCG, topn=topn)\n",
        "      for topn in [1, 3, 5, 10]\n",
        "  })\n",
        "\n",
        "  return metric_fns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SGJwvyrXk-Yj"
      },
      "source": [
        "### Putting It All Together\n",
        "\n",
        "We are now ready to put all of the components above together and create an `Estimator` that can be used to train and evaluate a model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "XoR9hRWHlCR4"
      },
      "outputs": [],
      "source": [
        "def get_estimator(hparams):\n",
        "  \"\"\"Create a ranking estimator.\n",
        "\n",
        "  Args:\n",
        "    hparams: (tf.contrib.training.HParams) a hyperparameters object.\n",
        "\n",
        "  Returns:\n",
        "    tf.learn `Estimator`.\n",
        "  \"\"\"\n",
        "  def _train_op_fn(loss):\n",
        "    \"\"\"Defines train op used in ranking head.\"\"\"\n",
        "    return tf.contrib.layers.optimize_loss(\n",
        "        loss=loss,\n",
        "        global_step=tf.train.get_global_step(),\n",
        "        learning_rate=hparams.learning_rate,\n",
        "        optimizer=\"Adagrad\")\n",
        "\n",
        "  ranking_head = tfr.head.create_ranking_head(\n",
        "      loss_fn=tfr.losses.make_loss_fn(_LOSS),\n",
        "      eval_metric_fns=eval_metric_fns(),\n",
        "      train_op_fn=_train_op_fn)\n",
        "\n",
        "  return tf.estimator.Estimator(\n",
        "      model_fn=tfr.model.make_groupwise_ranking_fn(\n",
        "          group_score_fn=make_score_fn(),\n",
        "          group_size=1,\n",
        "          transform_fn=None,\n",
        "          ranking_head=ranking_head),\n",
        "      params=hparams)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iJCwkuYJIuX0"
      },
      "source": [
        "Let us instantiate and initialize the `Estimator` we defined above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "7tD0aNuYU8LR"
      },
      "outputs": [],
      "source": [
        "hparams = tf.contrib.training.HParams(learning_rate=0.05)\n",
        "ranker = get_estimator(hparams)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kfxdz87CIuX3"
      },
      "source": [
        "Now that we have a correctly initialized `Estimator`, we will train a model using the training data. We encourage you to experiment with different number of steps here and below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "uXoA7xwuVmCD"
      },
      "outputs": [],
      "source": [
        "ranker.train(input_fn=lambda: input_fn(_TRAIN_DATA_PATH), steps=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LE7V4OZNIuX6"
      },
      "source": [
        "Finally, let us evaluate our model on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "xFI_16kcr4W4"
      },
      "outputs": [],
      "source": [
        "ranker.evaluate(input_fn=lambda: input_fn(_TEST_DATA_PATH), steps=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "viqssT46I3nC"
      },
      "source": [
        "### Visualization\n",
        "\n",
        "The train and evaluation steps above by default store checkpoints, metrics, and other useful information about your network to a temporary directory on disk. We encourage you to visualize this data using [Tensorboard](http://www.tensorflow.org/guide/summaries_and_tensorboard). In particular, you can launch Tensorboard and point it to where your model data is stored as follows:\n",
        "\n",
        "First, let's find out the path to the log directory created by the process above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "jj0gE4Y0IuX_"
      },
      "outputs": [],
      "source": [
        "ranker.model_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XuLpZ60pIuYC"
      },
      "source": [
        "Launch Tensorboard in shell using:\n",
        "\n",
        "$ tensorboard --logdir=\u003cranker.model_dir output\u003e"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "",
        "kind": "local"
      },
      "name": "TFRanking.ipynb",
      "provenance": [
        {
          "file_id": "1FRMQD7PASSHeiQ_x178qLh1vp3gLOq_N",
          "timestamp": 1540213305843
        }
      ],
      "version": "0.3.2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
